{
    "Name": "AWS Assessment",
    "Description": "Tracking [AWS Well-Architected Framework](http://d0.awsstatic.com/whitepapers/architecture/AWS_Well-Architected_Framework.pdf) compliance",
    "Background": "58e09e5905c611a8575de14c",
    "Labels": [{
        "Name": "Cost Optimization",
        "Color": "green"
    }, {
        "Name": "Operational Excellence",
        "Color": "orange"
    }, {
        "Name": "Performance Efficiency",
        "Color": "purple"
    }, {
        "Name": "Reliability",
        "Color": "blue"
    }, {
        "Name": "Security",
        "Color": "red"
    }, {
        "Name": "IAM",
        "Color": "sky"
    }, {
        "Name": "Detective Controls",
        "Color": "black"
    }, {
        "Name": "Infrastructure Protection",
        "Color": "pink"
    }, {
        "Name": "Data Protection",
        "Color": "lime"
    }, {
        "Name": "Incident Response",
        "Color": "yellow"
    }, {
        "Name": "Cost-Effective Resources",
        "Color": "sky"
    }, {
        "Name": "Matching Supply and Demand",
        "Color": "black"
    }, {
        "Name": "Expenditure Awareness",
        "Color": "lime"
    }, {
        "Name": "Optimizing Over Time",
        "Color": "pink"
    }, {
        "Name": "Preparation",
        "Color": "sky"
    }, {
        "Name": "Operations",
        "Color": "black"
    }, {
        "Name": "Responses",
        "Color": "pink"
    }, {
        "Name": "Selection",
        "Color": "lime"
    }, {
        "Name": "Review",
        "Color": "yellow"
    }, {
        "Name": "Monitoring",
        "Color": "sky"
    }, {
        "Name": "Foundations",
        "Color": "sky"
    }, {
        "Name": "Change Management",
        "Color": "pink"
    }, {
        "Name": "Failure Management",
        "Color": "black"
    }],
    "Lists": {
        "Security": [{
            "Name": "SEC 1: How are you protecting access to and use of the AWS root account credentials?",
            "Labels": ["Security", "IAM"],
            "Description": "The AWS root account credentials are similar to root or local admin in other operating systems and should be used very sparingly. The current best practice is to create AWS Identity and Access Management (IAM) users, associate them to an administrator group, and use the IAM user to manage the account. The AWS root account should not have API keys, should have a strong password, and should be associated with a hardware multi-factor authentication (MFA) device. This forces the only use of the root identity to be via the AWS Management Console and does not allow the root account to be used for application programming interface (API) calls. Note that some resellers or regions do not distribute or support the AWS root account credentials. \n## Best practices:\n  - **MFA and Minimal Use of Root** The AWS root account credentials are only used for only minimal required activities.\n  - **No use of Root**\n"
        }, {
            "Name": "SEC 2: How are you defining roles and responsibilities of system users to control human access to the AWS Management Console and API?",
            "Labels": ["Security", "IAM"],
            "Description": "The current best practice is for customers to segregate defined roles and responsibilities of system users by creating user groups. User groups can be defined using several different technologies: Identity and Access Management (IAM) groups, IAM roles for cross-account access, web identities, via Security Assertion Markup Language (SAML) integration (e.g., defining the roles in Active Directory), or by using a third-party solution (e.g., Okta, Ping Identity, or another custom technique) which usually integrates via either SAML or AWS Security Token Service (STS). Using a shared account is strongly discouraged. \n## Best practices:\n  - **Employee Life-Cycle Managed** Employee life-cycle policies are defined and enforced.\n  - **Least Privilege** Users, groups, and roles are clearly defined and granted only the minimum privileges needed to accomplish business requirements.\n"
        }, {
            "Name": "SEC 3: How are you limiting automated access to AWS resources?",
            "Labels": ["Security", "IAM"],
            "Description": "Systematic access should be defined in similar ways because user groups are created for people. For Amazon EC2 instances, these groups are called IAM roles for EC2. The current best practice is to use IAM roles for EC2 and an AWS SDK or CLI, which has built-in support for retrieving the IAM roles for EC2 credentials. Traditionally, user credentials are injected into EC2 instances, but hard coding the credential into scripts and source code is actively discouraged. \n## Best practices:\n  - **Static Credentials used for Automated Access** Stored these securely.\n  - **Dynamic Authentication for Automated Access** Manage using instance profiles or Amazon STS.\n"
        }, {
            "Name": "SEC 4: How are you capturing and analyzing logs?",
            "Labels": ["Security", "Detective Controls"],
            "Description": "Capturing logs is critical for investigating everything from performance to security incidents. The current best practice is for the logs to be periodically moved from the source either directly into a log processing system (e.g., CloudWatch Logs, Splunk, Papertrail, etc.) or stored in an Amazon S3 bucket for later processing based on business needs. Common sources of logs are AWS APIs and user-related logs (e.g., AWS CloudTrail), AWS service-specific logs (e.g., Amazon S3, Amazon CloudFront, etc.), operating system-generated logs, and third-party application-specific logs. You can use Amazon CloudWatch Logs to monitor, store, and access your log files from Amazon EC2 instances, AWS CloudTrail, or other sources.\n## Best practices:\n  - **Activity Monitored Appropriately Amazon CloudWatch logs, events, VPC flow logs, ELB logs, S3 bucket logs, etc.**\n  - **AWS Cloud Trail Enabled**\n  - **Monitored Operating System or Application Logs**\n"
        }, {
            "Name": "SEC 5: How are you enforcing network and host-level boundary protection?",
            "Labels": ["Security", "Infrastructure Protection"],
            "Description": "In on-premises data centers, a DMZ approach separates systems into trusted and untrusted zones using firewalls. On AWS, both stateful and stateless firewalls are used. Stateful firewalls are called security groups, and stateless firewalls are called network Access Control Lists (ACL) that protect the subnets in Amazon Virtual Private Cloud (VPC). The current best practice is to run a system in a VPC, and define the role-based security in security groups (e.g., web tier, app tier, etc.), and define the location-based security in network ACLs (e.g., an Elastic Load Balancing tier in one subnet per Availability Zone, web tier in another subnet per Availability Zone, etc.). \n## Best practices:\n  - **Controlled Network Traffic in VPC** For example, use firewalls, security groups, NACLS, a bastion host, etc.\n  - **Controlled Network Traffic at the Boundary** For example use AWS WAF, host based firewalls, security groups, NACLS, etc.\n"
        }, {
            "Name": "SEC 6. How are you leveraging AWS service level security features?",
            "Labels": ["Security", "Infrastructure Protection"],
            "Description": "AWS services may offer additional security features (e.g., Amazon S3 bucket policies, Amazon SQS, Amazon DynamoDB, KMS key policies, etc.). \n## Best practices:\n  - **Using Additional Features Where Appropriate**\n"
        }, {
            "Name": "SEC 7. How are you protecting the integrity of the operating systems on your Amazon EC2 instances?",
            "Labels": ["Security", "Infrastructure Protection"],
            "Description": "Another traditional control is to protect the integrity of the operating system. This is easily done in Amazon EC2 by using traditional host-based techniques (e.g., OSSEC, Tripwire, Trend Micro Deep Security, etc.). \n## Best practices:\n  - **File Integrity** File integrity controls are used for EC2 instances.\n  - **EC2 Intrusion Detection** Host-based intrusion detection controls are used for EC2 instances.\n  - **AWS Marketplace or Partner Solution** A solution from the AWS Marketplace or from an APN Partner.\n  - **Configuration Management Tool** Use of a custom Amazon Machine Image (AMI) or configuration management tools (such as Puppet or Chef) that are secured by default.\n"
        }, {
            "Name": "SEC 8. How are you classifying your data?",
            "Labels": ["Security", "Data Protection"],
            "Description": "Data classification provides a way to categorize organizational data based on levels of sensitivity. This includes what data types are available, where the data is located, access levels, and protection of the data (e.g. through encryption or access control). \n## Best Practices:\n  - **Using Data Classification Schema**\n  - **All data is Treated as Sensitive**\n"
        }, {
            "Name": "SEC 9. How are you encrypting and protecting your data at rest?",
            "Labels": ["Security", "Data Protection"],
            "Description": "A traditional security control is to encrypt data at rest. AWS supports this using both client-side (e.g., SDK-supported, operating system-supported, Windows Bitlocker, dm-crypt, Trend Micro SafeNet, etc.) and server-side (e.g., Amazon S3). You can also use Server-Side Encryption (SSE) and Amazon Elastic Block Store encrypted volumes.\n## Best Practices\n  - **Not Required** Data at rest encryption is not required\n  - **Encrypting at Rest**\n"
        }, {
            "Name": "SEC 10. How are you managing keys?",
            "Labels": ["Security", "Data Protection"],
            "Description": "Keys are secrets that should be protected, and an appropriate rotation policy should be defined and used. The best practice is to not hard-code these secrets into management scripts and applications, but it does often occur. \n## Best Practices:\n  - **AWS CloudHSM** Use AWS CloudHSM\n  - **Using AWS Service Controls** data at rest can be encrypted using AWS service-specific controls (e.g., Amazon S3 SSE, Amazon EBS encrypted volumes, Amazon Relational Database Service (RDS) Transparent Data Encryption (TDE), etc.).\n  - **Using Client Side** Data at rest is encrypted using client side techniques.\n  - **AWS Marketplace or Partner Solution** A solution from the AWS Marketplace or from an APN Partner. (e.g., SafeNet, TrendMicro, etc.).\n"
        }, {
            "Name": "SEC 11. How are you encrypting and protecting your data in transit?",
            "Labels": ["Security", "Data Protection"],
            "Description": "A best practice is to protect data in transit by using encryption. AWS supports using encrypted end-points for the service APIs. Additionally, customers can use various techniques within their Amazon EC2 instances. \n## Best Practices:\n  - **Not Required** Encryption not required on data in transit.\n  - **Encrypted Communications** TLS or equivalent is used for communication as appropriate.\n"
        }, {
            "Name": "SEC 12. How do you ensure you have the appropriate incident response?",
            "Labels": ["Security", "Incident Response"],
            "Description": "Putting in place the tools and access ahead of a security incident, then routinely practicing incident response will make sure the architecture is updated to accommodate timely investigation and recovery. \n## Best practices:\n  - **Pre-Provisioned Access** Infosec has the right access, or means to gain access quickly. This should be pre-provisioned so that an appropriate response can be made to an incident.\n  - **Pre-Deployed Tools** Infosec has the right tools pre-deployed into AWS so that an appropriate response can be made to an incident\n  - **Non-Production Game Days** Incident response simulations are conducted regularly in the non-production environment, and lessons learned are incorporated into the architecture and operations.\n  - **Production Game Days** Incident response simulations are conducted regularly in the production environment, and lessons learned are incorporated into the architecture and operations.\n"
        }],
        "Reliability": [{
            "Name": "REL 1. How do you manage AWS service limits for your accounts?",
            "Labels": ["Reliability", "Foundations"],
            "Description": "AWS accounts are provisioned with default service limits to prevent new users from accidentally provisioning more resources than they need. AWS customers should evaluate their AWS service needs and request appropriate changes to their limits for each region used. \n## Best practices:\n  - **Monitor and Manage Limits** Evaluate your potential usage on AWS, increase your regional limits appropriately, and allow planned growth in usage.\n  - **Set Up Automated Monitoring** Implement tools, e.g., SDKs, to alert you when thresholds are being approached.\n  - **Be Aware of Fixed Service Limits** Be aware of unchangeable service limits and architect around these.\n  - **Ensure There Is a Sufficient Gap Between Your Service Limit and Your Max Usage to Accommodate for Failover**\n  - **Service Limits are Considered Across All Relevant Accounts and Regions**\n"
        }, {
            "Name": "REL 2. How are you planning your network topology on AWS?",
            "Labels": ["Reliability", "Foundations"],
            "Description": "Applications can exist in one or more environments: EC2 Classic, VPC, or VPC by Default. Network considerations such as system connectivity, Elastic IP/public IP address management, VPC/private address management, and name resolution are fundamental to leveraging resources in the cloud. Wellplanned and documented deployments are essential to reduce the risk of overlap and contention. \n## Best Practices:\n  - **Connectivity Back to Data Center not Needed**\n  - **Highly Available Connectivity Between AWS and On-Premises Environment (as Applicable)** Multiple DX circuits, multiple VPN tunnels, AWS Marketplace appliances as applicable.\n  - **Highly Available Network Connectivity for the Users of the Workload** Highly available load balancing and/or proxy, DNS-based solution, AWS Marketplace appliances, etc.\n  - **Non-Overlapping Private IP Address Ranges** The use of IP address ranges and subnets in your virtual private cloud should not overlap each other, other cloud environments, or your on-premises environments.\n  - **IP Subnet Allocation** Individual Amazon VPC IP address ranges should be large enough to accommodate an application\u2019s requirements, including factoring in future expansion and allocation of IP addresses to subnets across Availability Zones.\n"
        }, {
            "Name": "REL 3. How does your system adapt to changes in demand?",
            "Labels": ["Reliability", "Change Management"],
            "Description": "A scalable system can provide elasticity to add and remove resources automatically so that they closely match the current demand at any given point in time.\n## Best practices:\n  - **Automated Scaling** Use automatically scalable services, e.g., Amazon S3, Amazon CloudFront, Auto Scaling, Amazon DynamoDB, AWS Elastic Beanstalk, etc.\n  - **Load Tested** Adopt a load testing methodology to measure if scaling activity will meet application requirements.\n"
        }, {
            "Name": "REL 4. How are you monitoring AWS resources?",
            "Labels": ["Reliability", "Change Management"],
            "Description": "Logs and metrics are a powerful tool for gaining insight into the health of your applications. You can configure your system to monitor logs and metrics and send notifications when thresholds are crossed or significant events occur. \n## Best practices:\n  - **Monitoring** Monitor your applications with Amazon CloudWatch or third-party tools.\n  - **Notification** Plan to receive notifications when significant events occur.\n  - **Automated Response** Use automation to take action when failure is detected, e.g., to replace failed components.\n"
        }, {
            "Name": "REL 5. How are you executing change?",
            "Labels": ["Reliability", "Change Management"],
            "Description": "Uncontrolled changes to your environment will make predictability of the effect of a change difficult. Controlled changes to provisioned AWS resources and applications is necessary to ensure that the applications and the operating environment are running known software and can be patched or replaced in a predictable manner. \n## Best practices:\n  - **Automated** Automate deployments and patching\n"
        }, {
            "Name": "REL 6. How are you backing up your data?",
            "Labels": ["Reliability", "Failure Management"],
            "Description": "Back up data, applications, and operating environments (defined as operating systems configured with applications) to meet requirements for mean time to recovery (MTTR) and recovery point objectives (RPO). \n## Best practices:\n  - **Automated Backups** Use AWS features, AWS Marketplace solutions, or third-party software to automate backups.\n  - **Periodic Recovery Testing** Validate that the backup process implementation meets RTO and RPO through a recovery test.\n"
        }, {
            "Name": "REL 7. How does your system withstand component failures?",
            "Labels": ["Reliability", "Failure Management"],
            "Description": "Do your applications have a requirement, implicit or explicit, for high availability and low mean time to recovery (MTTR)? If so, architect your applications for resiliency and distribute them to withstand outages. To achieve higher levels of availability, this distribution should span different physical locations. Architect individual layers (e.g., web server, database) for resiliency, which includes monitoring, self-healing, and notification of significant event disruption and failure. \n## Best practices:\n  - **Multi-AZ/Region** Distribute application load across multiple Availability Zones /Regions (e.g., DNS, ELB, Application Load Balancer, API Gateway)\n  - **Loosely Coupled Dependencies** For example use queuing systems, streaming systems, workflows, load balancers, etc.\n  - **Graceful Degradation** When a component\u2019s dependencies are unhealthy, the component itself does not report as unhealthy. It is capable of continuing to serve requests in a degraded manner.\n  - **Auto Healing** Use automated capabilities to detect failures and perform an action to remediate. Continuously monitor the health of your system and plan to receive notifications of any significant events.\n"
        }, {
            "Name": "REL 8. How are you testing for resiliency?",
            "Labels": ["Reliability", "Failure Management"],
            "Description": "When you test your resiliency you might find latent bugs that might only surface in production. Regularly exercising your procedures through game days will help your organization smoothly execute your procedures. \n## Best Practices:\n  - **Playbook** Have a playbook for failure scenarios.\n  - **Failure Injection** Regularly test failures (e.g., using Chaos Monkey), ensuring coverage of failure pathways.\n  - **Schedule Game Days**\n  - **Root Cause Analysis (RCA)** Perform reviews of system failures based on significant events to evaluate the architecture.\n"
        }, {
            "Name": "REL 9. How are you planning for disaster recovery?",
            "Labels": ["Reliability", "Failure Management"],
            "Description": "Data recovery (DR) is critical should restoration of data be required from backup methods. Your definition of and execution on the objectives, resources, locations, and functions of this data must align with RTO and RPO objectives. \n## Best practices:\n  - **Objectives Defined** Define RTO and RPO.\n  - **Disaster Recovery** Establish a DR strategy.\n  - **Configuration Drift** Ensure that Amazon Machine Images (AMIs) and the system configuration state are up-to-date at the DR site/region.\n  - **DR Tested and Validated** Regularly test failover to DR to ensure RTO and RPO are met.\n  - **Automated Recovery Implemented** Use AWS and/or third-party tools to automate system recovery.\n"
        }],
        "Performance Efficiency": [{
            "Name": "PERF 1. How do you select the best performing architecture?",
            "Labels": ["Performance Efficiency", "Selection"],
            "Description": "The optimal solution for a particular system will vary based on the kind of workload, often with multiple approaches combined. Well-architected systems use multiple solutions and enable different features to improve performance.\n## Best practices:\n  - **Benchmarking** Load test a known workload on AWS and use that to estimate the best selection.\n  - **Load Test** Deploy the latest version of your system on AWS using different resource types and sizes, use monitoring to capture performance metrics, and then make a selection based on a calculation of performance/cost.\n"
        }, {
            "Name": "PERF 2. How do you select your compute solution?",
            "Labels": ["Performance Efficiency", "Selection"],
            "Description": "The optimal compute solution for a particular system may vary based on application design, usage patterns, and configuration settings. Architectures may use different compute solutions for various components and enable different features to improve performance. Selecting the wrong compute solution for an architecture can lead to lower performance efficiency.\n## Best practices:\n  - **Consider Options** Consider the different options of using instances, containers, and functions to get the best performance.\n  - **Instance Configuration Options** If you use instances, consider configuration options such as family, instance sizes, and features (GPU, I/O, burstable).\n  - **Container Configuration Options** If you use containers, consider configuration options such as memory, CPU, and tenancy configuration of the container.\n  - **Function Configuration Options** If you use functions, consider configuration options such as memory, runtime, and state.\n  - **Elasticity** Use elasticity (e.g., Auto Scaling, Amazon EC2 Container Service (ECS), AWS Lambda) to meet changes in demand.\n"
        }, {
            "Name": "PERF 3. How do you select your storage solution?",
            "Labels": ["Performance Efficiency", "Selection"],
            "Description": "The optimal storage solution for a particular system will vary based on the kind of access method (block, file, or object), patterns of access (random or sequential), throughput required, frequency of access (online, offline, archival), frequency of update (WORM, dynamic), and availability and durability constraints. Well-architected systems use multiple storage solutions and enable different features to improve performance.\n## Best practices:\n  - **Consider Characteristics** Consider the different characteristics (e.g., shareable, file size, cache size, access patterns, latency, throughput, persistence of data) you require to select the services you need to use (Amazon S3, Amazon EBS, Amazon Elastic File System (EFS), EC2 instance store)\n  - **Consider Configuration Options** Considered configuration options such as PIOPS, SSD, magnetic, and Amazon S3 Transfer Acceleration.\n  - **Consider Access Patterns** Optimize for how you use storage systems based on access pattern (e.g., striping, key distribution, partitioning). \n"
        }, {
            "Name": "PERF 4. How do you select your database solution?",
            "Labels": ["Performance Efficiency", "Selection"],
            "Description": "The optimal database solution for a particular system can vary based on requirements for availability, consistency, partition tolerance, latency, durability, scalability and query capability. Many systems use different database solutions for various sub-systems and enable different features to improve performance. Selecting the wrong database solution and features for a system can lead to lower performance efficiency.\n## Best practices:\n  - **Consider Characteristics** Consider the different characteristics (e.g., availability, consistency, partition tolerance, latency, durability, scalability, query capability) so that you can select the most performant database approach to use (relational, No-SQL, warehouse, in-memory).\n  - **Consider Configuration Options** Consider configuration options such as storage optimization, database level settings, memory, and cache.\n  - **Consider Access Patterns** Optimize how you use database systems based on your access pattern (e.g., indexes, key distribution, partition, horizontal scaling).\n  - **Consider Other Approaches** Considered other approaches to providing queryable data such as search indexes, data warehouses, and big data.\n"
        }, {
            "Name": "PERF 5. How do you select your network solution?",
            "Labels": ["Performance Efficiency", "Selection"],
            "Description": "The optimal network solution for a particular system will vary based on latency, throughput requirements, and so on. Physical constraints such as user or onpremises resources will drive location options, which can be offset using edge techniques or resource placement.\n## Best practices:\n  - **Consider Location** Considered your location options (e.g., region, Availability Zone, placement groups, edge) to reduce network latency.\n  - **Consider Product Features** Consider product features (e.g., EC2 instance network capability, very high network instance types, Amazon EBS optimized instances, Amazon S3 Transfer Acceleration, Dynamic Amazon CloudFront) to optimize network traffic.\n  - **Consider Networking Features** Consider networking features (e.g., Amazon Route 53 latency routing, Amazon VPC endpoints, AWS Direct Connect) to reduce network distance or jitter.\n  - **Appropriate NACLS** Use the minimal set of NACLS to maintain network throughput.\n  - **Consider Encryption Offload** Consider using load balancing to offload encryption termination (TLS).\n  - **Consider protocols** Consider which protocols you need to optimize network performance.\n"
        }, {
            "Name": "PERF 6. How do you ensure that you continue to have the most appropriate resource type as new resource types and features are introduced?",
            "Labels": ["Performance Efficiency", "Review"],
            "Description": "When architecting solutions, there is a finite set of options that you can choose from. However, over time new technologies and approaches become available that could improve the performance of your architecture.\n## Best practices:\n  - **Review** Have a process for reviewing new resource types and sizes. Rerun performance tests to evaluate any improvements in performance efficiency.\n"
        }, {
            "Name": "PERF 7. How do you monitor your resources post-launch to ensure they are performing as expected?",
            "Labels": ["Performance Efficiency", "Monitoring"],
            "Description": "System performance can degrade over time due to internal and/or external factors. Monitoring the performance of systems allows you to identify this degradation and remediate internal or external factors (such as the operating system or application load). \n## Best practices:\n  - **Monitoring** Use Amazon CloudWatch, third-party, or custom monitoring tools to monitor performance.\n  - **Alarm-Based Notifications** Receive an automatic alert from your monitoring systems if metrics are out of safe bounds. \n  - **Trigger-Based Actions** Set alarms that cause automated actions to remediate or escalate issues. \n"
        }, {
            "Name": "PERF 8. How do you use tradeoffs to improve performance?",
            "Labels": ["Performance Efficiency", "Monitoring"],
            "Description": "When architecting solutions, actively thinking about tradeoffs will allow you to select an optimal approach. Often you can trade consistency, durability, and space versus time and latency to deliver higher performance.\n## Best practices:\n  - **Consider Services** Use services that improve performance, such as Amazon ElastiCache, Amazon CloudFront, and AWS Snowball.\n  - **Consider Patterns** Use patterns to improve performance, such as caching, read replicas, sharding, compression, and buffering.\n"
        }],
        "Cost Optimization": [{
            "Name": "COST 1. Are you considering cost when you select AWS services for your solution?",
            "Labels": ["Cost Optimization", "Cost-Effective Resources"],
            "Description": "Amazon EC2, Amazon EBS, Amazon S3, etc. are \u201cbuilding-block\u201d AWS services. Managed services such as Amazon RDS, Amazon DynamoDB, etc. are \u201chigher level\u201d AWS services. By selecting the appropriate building-blocks and managed services, you can optimize your architecture for cost. For example, using managed services, you can reduce or remove much of your administrative and operational overhead, freeing you to work on applications and business-related activities. \n## Best practices:\n  - **Select Services for Cost Reduction** Analyze services to see which ones you can use to reduce cost.\n  - **Optimize for License Costs**\n  - **Optimize Using Serverless and Container-Based Approach** Use of AWS Lambda, Amazon S3 websites, Amazon DynamoDB, and Amazon ECS to reduce cost.\n  - **Optimize Using Appropriate Storage Solutions** Use the most costeffective storage solution based on usage patterns (e.g., Amazon EBS cold storage, Amazon S3 Standard-Infrequent Access, Amazon Glacier, etc.).\n  - **Optimize Using Appropriate Databases** Use Amazon Relational Database Service (RDS) (Postgres, MySQL, SQL Server, Oracle Server) or Amazon DynamoDB (or other key-value stores, NoSQL alternatives) where it\u2019s appropriate.\n  - **Optimize Using Other Application-Level Services** Use Amazon Simple Queue Service (SQS), Amazon Simple Notification Service (SNS), and Amazon Simple Email Service (SES) where appropriate.\n"
        }, {
            "Name": "COST 2. Have you sized your resources to meet your cost targets?",
            "Labels": ["Cost Optimization", "Cost-Effective Resources"],
            "Description": "Ensure that you choose the appropriate AWS resource size for the task at hand. AWS encourages the use of benchmarking assessments to ensure that the type you choose is optimized for its workload. \n## Best practices:\n  - **Metrics Driven Resource Sizing** Leverage performance metrics to select the right size/type to optimize for cost. Appropriately provision throughput, sizing, and storage for services such as Amazon EC2, Amazon DynamoDB, Amazon EBS (provisioned IOPS), Amazon RDS, Amazon EMR, networking, etc.\n"
        }, {
            "Name": "COST 3. Have you selected the appropriate pricing model to meet your cost targets?",
            "Labels": ["Cost Optimization", "Cost-Effective Resources"],
            "Description": "Use the pricing model that is most appropriate for your workload to minimize expense. The optimal deployment could be fully On-Demand instances, a mix of On-Demand and Reserved Instances, or you might include Spot Instances, where applicable. \n## Best practices:\n  - **Reserved Capacity and Commit Deals** Regularly analyze usage and purchase Reserved Instances accordingly (e.g. Amazon EC2, Amazon DynamoDB, Amazon S3, Amazon CloudFront, etc.).\n  - **Spot** Use Spot Instances (e.g. Spot block, fleet) for select workloads (e.g., batch, EMR, etc.).\n  - **Consider Region Cost** Factor costs into region selection.\n"
        }, {
            "Name": "COST 4. How do you make sure your capacity matches but does not substantially exceed what you need?",
            "Labels": ["Cost Optimization", "Matching Supply and Demand"],
            "Description": "For an architecture that is balanced in terms of spend and performance, ensure that everything you pay for is used and avoid significantly underutilizing instances. A skewed utilization metric in either direction will have an adverse impact on your business in either operational costs (degraded performance due to over-utilization) or wasted AWS expenditures (due to over-provisioning). \n## Best practices:\n  - **Demand-Based Approach** Use Auto Scaling to respond to variable demand.\n  - **Buffer-Based Approach** Buffer work (e.g. using Amazon Kinesis or Amazon Simple Queue Service (SQS)) to defer work until you have sufficient capacity to process it.\n  - **Time-based approach** Examples of a time-based approach include following the sun, turning off Development and Test instances over the weekend, following quarterly or annual schedules (e.g., Black Friday).\n"
        }, {
            "Name": "COST 5. Did you consider data-transfer charges when designing your architecture?",
            "Labels": ["Cost Optimization", "Expenditure Awareness"],
            "Description": "Ensure that you monitor data-transfer charges so that you can make architectural decisions that might alleviate some of these costs. For example, if you are a content provider and have been serving content directly from an Amazon S3 bucket to your end users, you might be able to significantly reduce your costs if you push your content to the Amazon CloudFront content delivery network (CDN). Remember that a small yet effective architectural change can drastically reduce your operational costs. \n## Best practices:\n  - **Optimize** Architect to optimize data transfer (application design, WAN acceleration, Multi-AZ, region selection, etc.).\n  - **CDN** Use a CDN where applicable.\n  - **AWS Direct Connect** Analyze the situation and use AWS Direct Connect where applicable.\n"
        }, {
            "Name": "COST 6. How are you monitoring usage and spending?",
            "Labels": ["Cost Optimization", "Expenditure Awareness"],
            "Description": "Establish policies and procedures to monitor, control, and appropriately assign your costs. Leverage AWS-provided tools for visibility into who is using what\u2014 and at what cost. This will provide you with a deeper understanding of your business needs and your teams\u2019 operations. \n## Best practices:\n  - **Tag all resources** Tag all taggable resources to be able to correlate changes in your bill to changes in our infrastructure and usage.\n  - **Leverage Billing and Cost Management Tools** Have a standard process to load and interpret the Detailed Billing Reports or Cost Explorer. Monitor usage and spend regularly using Amazon CloudWatch or a third-party provider where applicable (examples: Cloudability, CloudCheckr, CloudHealth).\n  - **Notifications** Let key members of your team know if your spend moves outside of well-defined limits.\n  - **Finance Driven Charge Back/Show Back Method** Use this to allocate instances and resources to cost centers (e.g., using tagging).\n"
        }, {
            "Name": "COST 7. Do you decommission resources that you no longer need or stop resources that are temporarily not needed?",
            "Labels": ["Cost Optimization", "Expenditure Awareness"],
            "Description": "Implement change control and resource management from project inception to end-of-life so that you can identify necessary process changes or enhancements where appropriate. Work with AWS Support for recommendations on how to optimize your project for your workload: for example, when to use Auto Scaling, AWS OpsWorks, AWS Data Pipeline, or the different Amazon EC2 provisioning approaches or review Trusted Advisor cost optimization recommendations.\n## Best Practices:\n  - **Automated** Design your system to gracefully handle resource termination as you identify and decommission non-critical or unrequired resources with low utilization.\n  - **Defined Process** Have a process in place to identify and decommission orphaned resources.\n"
        }, {
            "Name": "COST 8. What access controls and procedures do you have in place to govern AWS usage?",
            "Labels": ["Cost Optimization", "Expenditure Awareness"],
            "Description": "Establish policies and mechanisms to make sure that appropriate costs are incurred while objectives are achieved. By employing a checks-and-balances approach through tagging and IAM controls, you can innovate without overspending. \n## Best practices:\n  - **Establish Groups and Roles (Example: Dev/Test/Prod)** Use governance mechanisms to control who can spin up instances and resources in each group. (This applies to AWS services or third-party solutions.)\n  - **Track Project Lifecycle** Track, measure, and audit the lifecycle of projects, teams, and environments to avoid using and paying for unnecessary resources.\n"
        }, {
            "Name": "COST 9. How do you manage and/or consider the adoption of new services?",
            "Labels": ["Cost Optimization", "Optimizing Over Time"],
            "Description": "As AWS releases new services and features, it is a best practice to review your existing architectural decisions to ensure they continue to be the most cost effective.\n## Best practices:\n  - **Establish a Cost Optimization Function**\n  - **Review** Have a process for reviewing new services, resource types, and sizes. Re-run performance tests to evaluate any reduction in cost. \n"
        }],
        "Operational Excellence": [{
            "Name": "OPS 1. What best practices for cloud operations are you using?",
            "Labels": ["Operational Excellence", "Preparation"],
            "Description": "Effective preparation is required to drive operational excellence. Using operations checklists ensures that your workloads are ready for production operation. The use of checklists prevents unintentional promotion to production without effective preparation.\n## Best practices:\n  - **Operational Checklist** Create an operational checklist that you use to evaluate if you are ready to operate the workload.\n  - **Proactive Plan** Have a proactive plan for events (e.g., marketing campaigns, flash sales) that prepares you for both opportunities and risks that could have a material impact on your business (e.g., reputation, finances).\n  - **Security Checklist** Create a security checklist that you can use to evaluate if you are ready to securely operate the workload (e.g., zero day, DDoS, compromised keys).\n"
        }, {
            "Name": "OPS 2. How are you doing configuration management for your workload?",
            "Labels": ["Operational Excellence", "Preparation"],
            "Description": "Environments, architecture, and the configuration parameters for resources within them, should be documented in a way that allows components to be easily identified for tracking and troubleshooting. Changes to configuration should also be trackable and automated.\n## Best practices:\n  - **Resource Tracking** Plan for ways to identify your resources and their function within the workload (e.g., use metadata, tagging).\n  - **Documentation** Document your architecture (e.g., infrastructure ascode, CMDB, diagrams, release notes).\n  - **Capture Operational Learnings** Captured operational learnings overtime (e.g., wiki, knowledge base, tickets).\n  - **Immutable Infrastructure** Establish an immutable infrastructure sothat you redeploy, you don\u2019t patch.\n  - **Automated Change Procedures** Automate your change procedures.\n  - **Configuration Management Database (CMDB)** Track all changesin a CMDB.\n"
        }, {
            "Name": "OPS 3. How are you evolving your workload while minimizing the impact of change?",
            "Labels": ["Operational Excellence", "Operations"],
            "Description": "Your focus should be on automation, small frequent changes, regular quality assurance testing, and defined mechanisms to track, audit, roll back, and review changes.\n## Best practices:\n  - **Deployment Pipeline** Put a CI/CD pipeline in place (e.g., source code repository, build systems, deployment and testing automation).\n  - **Release Management Process** Establish a release management process (e.g., manual or automated).\n  - **Small Incremental Changes** Ensure that you can release small incremental versions of system components.\n  - **Revertible Changes** Be prepared to revert changes that introduce operational issues (e.g., roll back, feature toggles).\n  - **Risk Mitigation Strategies** Use risk mitigation strategies such as Blue/Green, Canary, and A/B testing.\n"
        }, {
            "Name": "OPS 4. How do you monitor your workload to ensure it is operating as expected?",
            "Labels": ["Operational Excellence", "Operations"],
            "Description": "Your system can degrade over time due to internal and/or external factors. By monitoring the behavior of your systems, you can identify these factors of degradation and remediate them.\n## Best practices:\n  - **Monitoring** Use Amazon CloudWatch, third-party, or custom monitoring tools to monitor performance.\n  - **Aggregate Logs** Aggregate logs from multiple sources (e.g., application logs, AWS service-specific logs, VPC flow logs, CloudTrail).\n  - **Alarm-Based Notifications** Receive an automatic alert from your monitoring systems if metrics are out of safe bounds.\n  - **Trigger-Based Actions** Alarms cause automated actions to remediate or escalate issues.\n"
        }, {
            "Name": "OPS 5. How do you respond to unplanned operational events?",
            "Labels": ["Operational Excellence", "Responses"],
            "Description": "Be prepared to automate responses to unexpected operational events. This includes not just for alerting, but also mitigation, remediation, rollback, and recovery.\n# Best practices:\n  - **Playbook** Have a playbook that you follow (e.g., on call process, workflow chain, escalation process) and update regularly.\n  - **RCA Process** Have an RCA process to ensure that you can resolve, document, and fix issues so they do not happen in the future.\n  - **Automated Response** Handle unplanned operational events gracefully through automated responses (e.g., Auto Scaling, Support API).\n"
        }, {
            "Name": "OPS 6. How is escalation managed when responding to unplanned operational events?",
            "Labels": ["Operational Excellence", "Responses"],
            "Description": "Responses to unplanned operational events should follow a pre-defined playbook that includes stakeholders and the escalation process and procedures. Define escalation paths and include both functional and hierarchical escalation capabilities. Hierarchical escalation should be automated, and escalated priority should result in stakeholder notifications.\n## Best practices:\n  - **Appropriately Document and Provision** Put necessary stakeholders and systems in place for receiving alerts when escalations occur.\n  - **Functional Escalation with Queue-based Approach** Escalate between appropriate functional team queues based on priority, impact, and intake mechanisms.\n  - **Hierarchical Escalation** Use a demand- or time-based approach. As impact, scale, or time to resolution/recovery of incident increases, priority is escalated.\n  - **External Escalation Path** Include external support, AWS support, AWS Partners, and third-party support engagement in escalation paths.\n  - **Hierarchical Priority Escalation is Automated** When demand or time thresholds are passed, priority automatically escalates.\n"
        }],
        "In Progress": null,
        "Completed": null
    }
}